{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2*\n",
    "\n",
    "# Sprint Challenge - Neural Network Foundations\n",
    "\n",
    "Table of Problems\n",
    "\n",
    "1. [Defining Neural Networks](#Q1)\n",
    "2. [Chocolate Gummy Bears](#Q2)\n",
    "    - Perceptron\n",
    "    - Multilayer Perceptron\n",
    "4. [Keras MMP](#Q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Q1\"></a>\n",
    "## 1. Define the following terms:\n",
    "\n",
    "- **Neuron:** A Neuron is a number in the network in between synapses that informs the values of neurons in the layers before and after it through the weights of the synapses.\n",
    "- **Input Layer:** The input layer is the first layer of neurons activated in the network, and each neuron represents the observations of one feature of the dataset.\n",
    "- **Hidden Layer:** The hidden layers are the layers of neurons between the input layer and the output layer. Each neuron of a hidden layer is calculated by activating the sum of the weights x biases of the previous layer.\n",
    "- **Output Layer:** The output layer is the last layer of neurons activated in the network. Their neurons are calculated the same way as in a hidden layer, but the number of neurons in the ouput layer reflects the number of unique classes we want to predict. If MNIST for example, we use 10 output neurons. For regression, we use 1 output neuron, and the same for single classification.\n",
    "- **Activation:** The activation function is a function that normalizes the values of the neurons between a set range. Examples include tanh, sigmoid, and relu.\n",
    "- **Backpropagation:** Backpropogation is the process by which the error measured between the output values of the network and the actual values of the dataset are fed back through the weights and neurons to recalculate the initial weights of the neural network. This process is repeated through a set number of epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chocolate Gummy Bears <a id=\"Q2\"></a>\n",
    "\n",
    "Right now, you're probably thinking, \"yuck, who the hell would eat that?\". Great question. Your candy company wants to know too. And you thought I was kidding about the [Chocolate Gummy Bears](https://nuts.com/chocolatessweets/gummies/gummy-bears/milk-gummy-bears.html?utm_source=google&utm_medium=cpc&adpos=1o1&gclid=Cj0KCQjwrfvsBRD7ARIsAKuDvMOZrysDku3jGuWaDqf9TrV3x5JLXt1eqnVhN0KM6fMcbA1nod3h8AwaAvWwEALw_wcB). \n",
    "\n",
    "Let's assume that a candy company has gone out and collected information on the types of Halloween candy kids ate. Our candy company wants to predict the eating behavior of witches, warlocks, and ghosts -- aka costumed kids. They shared a sample dataset with us. Each row represents a piece of candy that a costumed child was presented with during \"trick\" or \"treat\". We know if the candy was `chocolate` (or not chocolate) or `gummy` (or not gummy). Your goal is to predict if the costumed kid `ate` the piece of candy. \n",
    "\n",
    "If both chocolate and gummy equal one, you've got a chocolate gummy bear on your hands!?!?!\n",
    "![Chocolate Gummy Bear](https://ed910ae2d60f0d25bcb8-80550f96b5feb12604f4f720bfefb46d.ssl.cf1.rackcdn.com/3fb630c04435b7b5-2leZuM7_-zoom.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "candy = pd.read_csv('chocolate_gummy_bears.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chocolate</th>\n",
       "      <th>gummy</th>\n",
       "      <th>ate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chocolate  gummy  ate\n",
       "0          0      1    1\n",
       "1          1      0    1\n",
       "2          0      1    1\n",
       "3          0      0    0\n",
       "4          1      1    0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "To make predictions on the `candy` dataframe. Build and train a Perceptron using numpy. Your target column is `ate` and your features: `chocolate` and `gummy`. Do not do any feature engineering. :P\n",
    "\n",
    "Once you've trained your model, report your accuracy. You will not be able to achieve more than ~50% with the simple perceptron. Explain why you could not achieve a higher accuracy with the *simple perceptron* architecture, because it's possible to achieve ~95% accuracy on this dataset. Provide your answer in markdown (and *optional* data anlysis code) after your perceptron implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = candy[['chocolate', 'gummy']].values\n",
    "y = candy['ate'].values\n",
    "print(X.shape)\n",
    "class Perceptron(object):\n",
    "    \n",
    "    def __init__(self, bias = 10, n_iter = 10):\n",
    "        self.bias = bias\n",
    "        self.n_iter = n_iter\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def sigmoid_prime(self, x):\n",
    "        sx = self.sigmoid(x)\n",
    "        return sx*(1-sx)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        weights = np.random.rand(X.shape[1], 1)\n",
    "        \n",
    "        for iteration in range(10000):\n",
    "    \n",
    "            # Weighted sum of inputs / weights\n",
    "            weighted_sum = np.dot(X, weights)\n",
    "\n",
    "            # Activate!\n",
    "            self.activated_output = self.sigmoid(weighted_sum)\n",
    "            # Cac error\n",
    "            error = y.reshape(-1, 1) - self.activated_output\n",
    "\n",
    "            adjustments = error * self.sigmoid_prime(self.activated_output)\n",
    "            # Update the Weights\n",
    "            weights += np.dot(X.T, adjustments)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.activated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tally\\.virtualenvs\\DS-Unit-4-Sprint-2-Neural-Networks-psAJyakQ\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "pct = Perceptron()\n",
    "pct.fit(X, y)\n",
    "y_pred = pct.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1)\n",
      "(10000, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7229"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(y_pred.shape)\n",
    "print(y.reshape(-1, 1).shape)\n",
    "accuracy_score(y.reshape(-1, 1), y_pred.round())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This single-layer perceptron doesn't produce useful accuracy because single-layer perceptrons are terrible at the XOR problem, a linerarly non-separable problem. A multi-layer perceptron should fare better because it can implement differentiable gradient descent. I'm surprised that i got 72% accuracy. I think it was because I rounded my NN outputs. Otherwise sklearn accuracy score didn't like that I mixed binary with float NN outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron <a id=\"Q3\"></a>\n",
    "\n",
    "Using the sample candy dataset, implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. Your Multilayer Perceptron should be implemented in Numpy. \n",
    "Your network must have one hidden layer.\n",
    "\n",
    "Once you've trained your model, report your accuracy. Explain why your MLP's performance is considerably better than your simple perceptron's on the candy dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    def __init__(self):        \n",
    "        #Define Hyperparameters\n",
    "        self.inputLayerSize = 2\n",
    "        self.outputLayerSize = 1\n",
    "        self.hiddenLayerSize = 3\n",
    "        \n",
    "        #Weights (parameters)\n",
    "        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #Propogate inputs though network\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3) \n",
    "        return yHat\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        #Apply sigmoid activation function to scalar, vector, or matrix\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self,z):\n",
    "        #Gradient of sigmoid\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "    \n",
    "    def costFunction(self, X, y):\n",
    "        #Compute cost for given X,y, use weights already stored in class.\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5*sum((y-self.yHat)**2)\n",
    "        return J\n",
    "        \n",
    "    def costFunctionPrime(self, X, y):\n",
    "        #Compute derivative with respect to W and W2 for a given X and y:\n",
    "        self.yHat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2)  \n",
    "        \n",
    "        return dJdW1, dJdW2\n",
    "    \n",
    "    #Helper Functions for interacting with other classes:\n",
    "    def getParams(self):\n",
    "        #Get W1 and W2 unrolled into vector:\n",
    "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
    "        return params\n",
    "    \n",
    "    def setParams(self, params):\n",
    "        #Set W1 and W2 using single paramater vector.\n",
    "        W1_start = 0\n",
    "        W1_end = self.hiddenLayerSize * self.inputLayerSize\n",
    "        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize , self.hiddenLayerSize))\n",
    "        W2_end = W1_end + self.hiddenLayerSize*self.outputLayerSize\n",
    "        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize, self.outputLayerSize))\n",
    "        \n",
    "    def computeGradients(self, X, y):\n",
    "        dJdW1, dJdW2 = self.costFunctionPrime(X, y)\n",
    "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))\n",
    "    \n",
    "from scipy import optimize\n",
    "class trainer(object):\n",
    "    def __init__(self, N):\n",
    "        #Make Local reference to network:\n",
    "        self.N = N\n",
    "        \n",
    "    def callbackF(self, params):\n",
    "        self.N.setParams(params)\n",
    "        self.J.append(self.N.costFunction(self.X, self.y))   \n",
    "        \n",
    "    def costFunctionWrapper(self, params, X, y):\n",
    "        self.N.setParams(params)\n",
    "        cost = self.N.costFunction(X, y)\n",
    "        grad = self.N.computeGradients(X,y)\n",
    "        \n",
    "        return cost, grad\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        #Make an internal variable for the callback function:\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        #Make empty list to store costs:\n",
    "        self.J = []\n",
    "        \n",
    "        params0 = self.N.getParams()\n",
    "\n",
    "        options = {'maxiter': 200, 'disp' : True}\n",
    "        _res = optimize.minimize(self.costFunctionWrapper, params0, jac=True, method='BFGS', \\\n",
    "                                 args=(X, y), options=options, callback=self.callbackF)\n",
    "\n",
    "        self.N.setParams(_res.x)\n",
    "        self.optimizationResults = _res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 256.295819\n",
      "         Iterations: 91\n",
      "         Function evaluations: 226\n",
      "         Gradient evaluations: 214\n"
     ]
    }
   ],
   "source": [
    "NN = Neural_Network()\n",
    "T = trainer(NN)\n",
    "T.train(X,y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output: \n",
      "[[0.94741067]\n",
      " [0.9477702 ]\n",
      " [0.94741067]\n",
      " ...\n",
      " [0.94741067]\n",
      " [0.94741067]\n",
      " [0.9477702 ]]\n",
      "Loss: \n",
      "0.44874084240820267\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted Output: \\n\" + str(NN.forward(X))) \n",
    "print(\"Loss: \\n\" + str(np.mean(np.square(y - NN.forward(X))))) # mean sum squared loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S. Don't try candy gummy bears. They're disgusting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keras MMP <a id=\"Q3\"></a>\n",
    "\n",
    "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
    "Use the Heart Disease Dataset (binary classification)\n",
    "Use an appropriate loss function for a binary classification task\n",
    "Use an appropriate activation function on the final layer of your network.\n",
    "Train your model using verbose output for ease of grading.\n",
    "Use GridSearchCV or RandomSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
    "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "You must hyperparameter tune at least 3 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>154</td>\n",
       "      <td>232</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>164</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>221</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>164</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>138</td>\n",
       "      <td>257</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>156</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>220</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>125</td>\n",
       "      <td>273</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "287   57    1   1       154   232    0        0      164      0      0.0   \n",
       "77    59    1   1       140   221    0        1      164      1      0.0   \n",
       "47    47    1   2       138   257    0        0      156      0      0.0   \n",
       "68    44    1   1       120   220    0        1      170      0      0.0   \n",
       "33    54    1   2       125   273    0        0      152      0      0.5   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "287      2   1     2       0  \n",
       "77       2   0     2       1  \n",
       "47       2   0     2       1  \n",
       "68       2   0     2       1  \n",
       "33       0   1     2       1  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
    "df = df.sample(frac=1)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.columns[1:len(df)-1]\n",
    "target = 'target'\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_clf(hidden_layers=[64, 64, 64], dropout_rate=0, optimizer='adam', activation='relu', input_dim=12545, output_dim=1):\n",
    "    model = Sequential()\n",
    "    for index, layers in enumerate(hidden_layers):\n",
    "        #Input layer\n",
    "        if index == 0:\n",
    "            model.add(Dense(layers, input_dim=input_dim, activation=activation))\n",
    "        #Hidden layers\n",
    "        else:\n",
    "            model.add(Dense(layers, activation=activation))\n",
    "        #Dropout\n",
    "        if dropout_rate:\n",
    "            model.add(Dropout(rate=dropout_rate))\n",
    "    #Output layer\n",
    "    if output_dim > 2:\n",
    "        model.add(Dense(output_dim, activation='softmax'))\n",
    "        loss='categorical_crossentropy'\n",
    "    else:\n",
    "        model.add(Dense(output_dim, activation=activation))\n",
    "        loss='binary_crossentropy'\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, validation_split=.1, verbose=0, batch_size=40, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 0s 131us/sample - loss: 0.0021 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'acc: 1.0'"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{model.metrics_names[1]}: {scores[1]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=X_train.shape[1]\n",
    "output_dim=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_keras = KerasClassifier(build_fn=keras_clf,\n",
    "                              input_dim=input_dim,\n",
    "                              output_dim=output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_epochs = range(10, 30, 4)\n",
    "range_layers = [[64, 64, 64, 64], [32, 32, 32, 32, 32], [100, 100, 100]]\n",
    "range_batchsize = range(20, 20000, 2000)\n",
    "range_optimizer = ['sgd', 'rmsprop', 'adagrad', 'adadelta', 'adam', 'adamax', 'nadam']\n",
    "range_activations = ['relu', 'tanh', 'sigmoid']\n",
    "range_dropouts = [n/100 for n in range(20, 50)]\n",
    "\n",
    "keras_params_options = {\n",
    "    'epochs': range_epochs,\n",
    "    'dropout_rate': range_dropouts,\n",
    "    'batch_size': range_batchsize,\n",
    "    'optimizer': range_optimizer,\n",
    "    'activation': range_activations,\n",
    "#     'hidden_layers': range_layers\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "161/161 [==============================] - 0s 2ms/sample - loss: 4.7002 - acc: 0.4224\n",
      "Epoch 2/14\n",
      "161/161 [==============================] - 0s 31us/sample - loss: 3.6081 - acc: 0.4348\n",
      "Epoch 3/14\n",
      "161/161 [==============================] - 0s 31us/sample - loss: 2.9073 - acc: 0.4907\n",
      "Epoch 4/14\n",
      "161/161 [==============================] - 0s 43us/sample - loss: 3.5912 - acc: 0.5031\n",
      "Epoch 5/14\n",
      "161/161 [==============================] - 0s 50us/sample - loss: 2.8197 - acc: 0.4596\n",
      "Epoch 6/14\n",
      "161/161 [==============================] - 0s 37us/sample - loss: 2.8889 - acc: 0.5714\n",
      "Epoch 7/14\n",
      "161/161 [==============================] - 0s 43us/sample - loss: 2.8603 - acc: 0.5590\n",
      "Epoch 8/14\n",
      "161/161 [==============================] - 0s 25us/sample - loss: 2.3743 - acc: 0.5839\n",
      "Epoch 9/14\n",
      "161/161 [==============================] - 0s 31us/sample - loss: 2.3850 - acc: 0.6398\n",
      "Epoch 10/14\n",
      "161/161 [==============================] - 0s 37us/sample - loss: 2.2708 - acc: 0.5714\n",
      "Epoch 11/14\n",
      "161/161 [==============================] - 0s 43us/sample - loss: 2.3448 - acc: 0.6149\n",
      "Epoch 12/14\n",
      "161/161 [==============================] - 0s 31us/sample - loss: 2.3556 - acc: 0.6398\n",
      "Epoch 13/14\n",
      "161/161 [==============================] - 0s 37us/sample - loss: 2.1349 - acc: 0.6646\n",
      "Epoch 14/14\n",
      "161/161 [==============================] - 0s 48us/sample - loss: 1.6667 - acc: 0.6211\n",
      "81/81 [==============================] - 0s 4ms/sample - loss: 0.3689 - acc: 0.9012\n",
      "Epoch 1/14\n",
      "161/161 [==============================] - 0s 2ms/sample - loss: 4.6492 - acc: 0.4099\n",
      "Epoch 2/14\n",
      "161/161 [==============================] - 0s 31us/sample - loss: 4.8317 - acc: 0.4534\n",
      "Epoch 3/14\n",
      "161/161 [==============================] - 0s 37us/sample - loss: 3.7721 - acc: 0.3975\n",
      "Epoch 4/14\n",
      "161/161 [==============================] - 0s 56us/sample - loss: 4.4006 - acc: 0.4161\n",
      "Epoch 5/14\n",
      "161/161 [==============================] - 0s 43us/sample - loss: 3.6263 - acc: 0.4472\n",
      "Epoch 6/14\n",
      "161/161 [==============================] - 0s 37us/sample - loss: 3.4778 - acc: 0.4907\n",
      "Epoch 7/14\n",
      "161/161 [==============================] - 0s 37us/sample - loss: 2.3572 - acc: 0.5217\n",
      "Epoch 8/14\n",
      "161/161 [==============================] - 0s 44us/sample - loss: 2.8487 - acc: 0.4845\n",
      "Epoch 9/14\n",
      "161/161 [==============================] - 0s 31us/sample - loss: 2.9997 - acc: 0.5280\n",
      "Epoch 10/14\n",
      "161/161 [==============================] - 0s 30us/sample - loss: 3.4209 - acc: 0.4845\n",
      "Epoch 11/14\n",
      "161/161 [==============================] - 0s 56us/sample - loss: 2.5664 - acc: 0.5342\n",
      "Epoch 12/14\n",
      "161/161 [==============================] - 0s 37us/sample - loss: 2.7898 - acc: 0.5652\n",
      "Epoch 13/14\n",
      "161/161 [==============================] - 0s 31us/sample - loss: 2.3578 - acc: 0.5901\n",
      "Epoch 14/14\n",
      "161/161 [==============================] - 0s 41us/sample - loss: 2.2042 - acc: 0.6025\n",
      "81/81 [==============================] - 0s 3ms/sample - loss: 0.7018 - acc: 0.8148\n",
      "Epoch 1/14\n",
      "162/162 [==============================] - 0s 3ms/sample - loss: 4.5771 - acc: 0.4753\n",
      "Epoch 2/14\n",
      "162/162 [==============================] - 0s 43us/sample - loss: 4.3223 - acc: 0.5494\n",
      "Epoch 3/14\n",
      "162/162 [==============================] - 0s 37us/sample - loss: 4.4807 - acc: 0.4815\n",
      "Epoch 4/14\n",
      "162/162 [==============================] - 0s 36us/sample - loss: 4.4278 - acc: 0.4753\n",
      "Epoch 5/14\n",
      "162/162 [==============================] - 0s 24us/sample - loss: 4.3470 - acc: 0.4074\n",
      "Epoch 6/14\n",
      "162/162 [==============================] - 0s 43us/sample - loss: 4.4982 - acc: 0.4568\n",
      "Epoch 7/14\n",
      "162/162 [==============================] - 0s 51us/sample - loss: 2.8453 - acc: 0.5926\n",
      "Epoch 8/14\n",
      "162/162 [==============================] - 0s 37us/sample - loss: 3.1605 - acc: 0.5185\n",
      "Epoch 9/14\n",
      "162/162 [==============================] - 0s 31us/sample - loss: 2.9124 - acc: 0.5617\n",
      "Epoch 10/14\n",
      "162/162 [==============================] - 0s 49us/sample - loss: 3.0509 - acc: 0.5679\n",
      "Epoch 11/14\n",
      "162/162 [==============================] - 0s 44us/sample - loss: 2.9984 - acc: 0.5556\n",
      "Epoch 12/14\n",
      "162/162 [==============================] - 0s 49us/sample - loss: 2.4041 - acc: 0.6111\n",
      "Epoch 13/14\n",
      "162/162 [==============================] - 0s 31us/sample - loss: 2.2046 - acc: 0.5988\n",
      "Epoch 14/14\n",
      "162/162 [==============================] - 0s 56us/sample - loss: 2.9359 - acc: 0.5741\n",
      "80/80 [==============================] - 0s 3ms/sample - loss: 0.4126 - acc: 0.8125\n",
      "Epoch 1/14\n",
      "161/161 [==============================] - 0s 3ms/sample - loss: 6.7513 - acc: 0.2919\n",
      "Epoch 2/14\n",
      "161/161 [==============================] - 0s 37us/sample - loss: 6.2058 - acc: 0.2981\n",
      "Epoch 3/14\n",
      "161/161 [==============================] - 0s 31us/sample - loss: 6.7292 - acc: 0.3106\n",
      "Epoch 4/14\n",
      "161/161 [==============================] - 0s 43us/sample - loss: 5.8854 - acc: 0.2981\n",
      "Epoch 5/14\n",
      "161/161 [==============================] - 0s 50us/sample - loss: 6.7336 - acc: 0.3292\n",
      "Epoch 6/14\n",
      "161/161 [==============================] - 0s 43us/sample - loss: 6.3404 - acc: 0.3292\n",
      "Epoch 7/14\n",
      "161/161 [==============================] - 0s 56us/sample - loss: 6.0592 - acc: 0.3043\n",
      "Epoch 8/14\n",
      "161/161 [==============================] - 0s 40us/sample - loss: 7.0983 - acc: 0.2919\n",
      "Epoch 9/14\n",
      "161/161 [==============================] - 0s 34us/sample - loss: 5.9615 - acc: 0.3354\n",
      "Epoch 10/14\n",
      "161/161 [==============================] - 0s 39us/sample - loss: 6.2635 - acc: 0.3106\n",
      "Epoch 11/14\n",
      "161/161 [==============================] - 0s 43us/sample - loss: 6.8197 - acc: 0.3043\n",
      "Epoch 12/14\n",
      "161/161 [==============================] - 0s 43us/sample - loss: 6.3394 - acc: 0.2981\n",
      "Epoch 13/14\n",
      "161/161 [==============================] - 0s 43us/sample - loss: 5.9996 - acc: 0.3727\n",
      "Epoch 14/14\n",
      "161/161 [==============================] - 0s 43us/sample - loss: 6.6492 - acc: 0.3354\n",
      "81/81 [==============================] - 0s 3ms/sample - loss: 8.2060 - acc: 0.2716\n",
      "Epoch 1/14\n",
      "161/161 [==============================] - 1s 3ms/sample - loss: 6.9659 - acc: 0.2733\n",
      "Epoch 2/14\n",
      "161/161 [==============================] - 0s 43us/sample - loss: 7.2388 - acc: 0.2857\n",
      "Epoch 3/14\n",
      "161/161 [==============================] - 0s 43us/sample - loss: 5.9978 - acc: 0.3106\n",
      "Epoch 4/14\n",
      "161/161 [==============================] - 0s 46us/sample - loss: 6.7336 - acc: 0.2609\n",
      "Epoch 5/14\n",
      "161/161 [==============================] - 0s 62us/sample - loss: 7.4298 - acc: 0.2733\n",
      "Epoch 6/14\n",
      "161/161 [==============================] - 0s 43us/sample - loss: 6.6506 - acc: 0.2422\n",
      "Epoch 7/14\n",
      "161/161 [==============================] - 0s 43us/sample - loss: 6.4923 - acc: 0.3043\n",
      "Epoch 8/14\n",
      "161/161 [==============================] - 0s 37us/sample - loss: 7.4083 - acc: 0.2795\n",
      "Epoch 9/14\n",
      "161/161 [==============================] - 0s 47us/sample - loss: 6.9570 - acc: 0.2857\n",
      "Epoch 10/14\n",
      "161/161 [==============================] - 0s 31us/sample - loss: 7.1055 - acc: 0.3043\n",
      "Epoch 11/14\n",
      "161/161 [==============================] - 0s 37us/sample - loss: 7.1971 - acc: 0.2609\n",
      "Epoch 12/14\n",
      "161/161 [==============================] - 0s 37us/sample - loss: 7.3415 - acc: 0.3168\n",
      "Epoch 13/14\n",
      "161/161 [==============================] - 0s 43us/sample - loss: 7.0697 - acc: 0.2547\n",
      "Epoch 14/14\n",
      "161/161 [==============================] - 0s 37us/sample - loss: 6.6136 - acc: 0.2360\n",
      "81/81 [==============================] - 0s 4ms/sample - loss: 7.3731 - acc: 0.2593\n",
      "Epoch 1/14\n",
      "162/162 [==============================] - 0s 3ms/sample - loss: 6.8184 - acc: 0.3519\n",
      "Epoch 2/14\n",
      "162/162 [==============================] - 0s 43us/sample - loss: 6.4596 - acc: 0.3025\n",
      "Epoch 3/14\n",
      "162/162 [==============================] - 0s 31us/sample - loss: 6.1139 - acc: 0.3580\n",
      "Epoch 4/14\n",
      "162/162 [==============================] - 0s 37us/sample - loss: 6.2379 - acc: 0.3704\n",
      "Epoch 5/14\n",
      "162/162 [==============================] - 0s 34us/sample - loss: 6.9817 - acc: 0.2778\n",
      "Epoch 6/14\n",
      "162/162 [==============================] - 0s 31us/sample - loss: 6.5109 - acc: 0.3395\n",
      "Epoch 7/14\n",
      "162/162 [==============================] - 0s 49us/sample - loss: 5.9599 - acc: 0.3642\n",
      "Epoch 8/14\n",
      "162/162 [==============================] - 0s 31us/sample - loss: 6.6363 - acc: 0.3457\n",
      "Epoch 9/14\n",
      "162/162 [==============================] - 0s 49us/sample - loss: 6.6981 - acc: 0.2963\n",
      "Epoch 10/14\n",
      "162/162 [==============================] - 0s 31us/sample - loss: 6.8182 - acc: 0.3086\n",
      "Epoch 11/14\n",
      "162/162 [==============================] - 0s 31us/sample - loss: 6.8232 - acc: 0.3272\n",
      "Epoch 12/14\n",
      "162/162 [==============================] - 0s 36us/sample - loss: 6.2058 - acc: 0.3333\n",
      "Epoch 13/14\n",
      "162/162 [==============================] - 0s 44us/sample - loss: 6.2802 - acc: 0.3580\n",
      "Epoch 14/14\n",
      "162/162 [==============================] - 0s 37us/sample - loss: 6.3079 - acc: 0.2901\n",
      "80/80 [==============================] - 0s 4ms/sample - loss: 7.3236 - acc: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:   23.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "242/242 [==============================] - 0s 2ms/sample - loss: 3.8765 - acc: 0.4463\n",
      "Epoch 2/14\n",
      "242/242 [==============================] - 0s 25us/sample - loss: 4.2300 - acc: 0.4215\n",
      "Epoch 3/14\n",
      "242/242 [==============================] - 0s 25us/sample - loss: 3.8795 - acc: 0.5331\n",
      "Epoch 4/14\n",
      "242/242 [==============================] - 0s 25us/sample - loss: 3.9585 - acc: 0.5041\n",
      "Epoch 5/14\n",
      "242/242 [==============================] - 0s 29us/sample - loss: 3.3084 - acc: 0.5496\n",
      "Epoch 6/14\n",
      "242/242 [==============================] - 0s 25us/sample - loss: 3.6498 - acc: 0.4628\n",
      "Epoch 7/14\n",
      "242/242 [==============================] - 0s 29us/sample - loss: 3.1858 - acc: 0.5289\n",
      "Epoch 8/14\n",
      "242/242 [==============================] - 0s 25us/sample - loss: 2.7379 - acc: 0.5620\n",
      "Epoch 9/14\n",
      "242/242 [==============================] - 0s 29us/sample - loss: 2.6529 - acc: 0.5537\n",
      "Epoch 10/14\n",
      "242/242 [==============================] - 0s 21us/sample - loss: 2.5166 - acc: 0.5909\n",
      "Epoch 11/14\n",
      "242/242 [==============================] - 0s 33us/sample - loss: 2.9054 - acc: 0.5165\n",
      "Epoch 12/14\n",
      "242/242 [==============================] - 0s 45us/sample - loss: 2.4658 - acc: 0.6405\n",
      "Epoch 13/14\n",
      "242/242 [==============================] - 0s 35us/sample - loss: 2.1033 - acc: 0.6446\n",
      "Epoch 14/14\n",
      "242/242 [==============================] - 0s 27us/sample - loss: 2.4496 - acc: 0.6116\n"
     ]
    }
   ],
   "source": [
    "search = RandomizedSearchCV(model_keras,\n",
    "                             param_distributions=keras_params_options,\n",
    "                             cv=3,\n",
    "                             n_iter=2,\n",
    "                             n_jobs=1,\n",
    "                             verbose=1)\n",
    "best = search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 0s 5ms/sample - loss: 0.6518 - acc: 0.8033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8032787"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best.best_estimator_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python (NN)",
   "language": "python",
   "name": "other-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
